{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e2ec9b",
   "metadata": {},
   "source": [
    "# Pr√©diction de l'Attrition Client Bancaire : Pipeline de Machine Learning Distribu√© avec Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d094a45",
   "metadata": {},
   "source": [
    "* imports : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1208be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671339f8",
   "metadata": {},
   "source": [
    "## üü¢ Configuration et Initialisation de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6928d47",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Configuration de l‚Äôenvironnement Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd9882",
   "metadata": {},
   "source": [
    "#### - Installer et configurer PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15bd1d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install pyspark==3.5.0\\npip install pymongo\\npip install findspark\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installation des d√©pendances ( ex√©cuter dans le terminal)\n",
    "\"\"\"\n",
    "pip install pyspark==3.5.0\n",
    "pip install pymongo\n",
    "pip install findspark\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142cb0ed",
   "metadata": {},
   "source": [
    "#### - Cr√©er et tester une SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eaff229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Banking_Churn_Prediction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e6ec6",
   "metadata": {},
   "source": [
    "#### - V√©rifier la version et les param√®tres d‚Äôex√©cution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e987a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialis√©e avec succ√®s !\n",
      "Version de Spark : 4.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ SparkSession initialis√©e avec succ√®s !\")\n",
    "print(f\"Version de Spark : {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c82d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  Param√®tres Spark en cours d'ex√©cution :\n",
      "spark.rdd.compress = True\n",
      "spark.hadoop.fs.s3a.vectored.read.min.seek.size = 128K\n",
      "spark.app.startTime = 1762191534888\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "spark.sql.artifact.isolation.enabled = false\n",
      "spark.master = local[*]\n",
      "spark.app.name = Banking_Churn_Prediction\n",
      "spark.executor.id = driver\n",
      "spark.submit.pyFiles = \n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "spark.hadoop.fs.s3a.vectored.read.max.merged.size = 2M\n",
      "spark.submit.deployMode = client\n",
      "spark.app.id = local-1762191534993\n",
      "spark.app.submitTime = 1762186423927\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.driver.port = 60970\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.driver.host = Bouchra\n"
     ]
    }
   ],
   "source": [
    "# Afficher quelques configurations importantes\n",
    "print(\"\\n‚öôÔ∏è  Param√®tres Spark en cours d'ex√©cution :\")\n",
    "for key, value in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{key} = {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81c612",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e16ce",
   "metadata": {},
   "source": [
    "#### - Charger les donn√©es brutes au format CSV dans un DataFrame Spark avec : spark.read.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1f13152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es charg√©es avec succ√©s !\n",
      "Nombre de lignes ; 10000\n",
      "Nombre de colonnes : 14\n"
     ]
    }
   ],
   "source": [
    "df_clients = spark.read.csv(\n",
    "    \"../data/dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    nullValue=\"NA\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Donn√©es charg√©es avec succ√©s !\")\n",
    "print(f\"Nombre de lignes ; {df_clients.count()}\")\n",
    "print(f\"Nombre de colonnes : {len(df_clients.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a09a5527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[RowNumber: int, CustomerId: int, Surname: string, CreditScore: int, Geography: string, Gender: string, Age: int, Tenure: int, Balance: double, NumOfProducts: int, HasCrCard: int, IsActiveMember: int, EstimatedSalary: double, Exited: int]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649eb9f",
   "metadata": {},
   "source": [
    "#### - Afficher le sch√©ma des colonnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e349d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sch√©ma des colonnes : \n",
      "root\n",
      " |-- RowNumber: integer (nullable = true)\n",
      " |-- CustomerId: integer (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- CreditScore: integer (nullable = true)\n",
      " |-- Geography: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: integer (nullable = true)\n",
      " |-- HasCrCard: integer (nullable = true)\n",
      " |-- IsActiveMember: integer (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sch√©ma des colonnes : \")\n",
    "df_clients.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a3b2e",
   "metadata": {},
   "source": [
    "#### - Afficher un aper√ßu des premi√®res lignes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37526421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aper√ßu des donn√©es :\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId|Surname |CreditScore|Geography|Gender|Age|Tenure|Balance  |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|1        |15634602  |Hargrave|619        |France   |Female|42 |2     |0.0      |1            |1        |1             |101348.88      |1     |\n",
      "|2        |15647311  |Hill    |608        |Spain    |Female|41 |1     |83807.86 |1            |0        |1             |112542.58      |0     |\n",
      "|3        |15619304  |Onio    |502        |France   |Female|42 |8     |159660.8 |3            |1        |0             |113931.57      |1     |\n",
      "|4        |15701354  |Boni    |699        |France   |Female|39 |1     |0.0      |2            |0        |0             |93826.63       |0     |\n",
      "|5        |15737888  |Mitchell|850        |Spain    |Female|43 |2     |125510.82|1            |1        |1             |79084.1        |0     |\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Aper√ßu des donn√©es :\")\n",
    "df_clients.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ac029",
   "metadata": {},
   "source": [
    "* stoper spark : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cf0b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22056c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
