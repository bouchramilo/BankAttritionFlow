{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232bc2ac",
   "metadata": {},
   "source": [
    "# Pr√©diction de l'Attrition Client Bancaire : Pipeline de Machine Learning Distribu√© avec Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6b540",
   "metadata": {},
   "source": [
    "* imports : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffaca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf342f3",
   "metadata": {},
   "source": [
    "## üü¢ Construction du Pipeline de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a175c8e",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ R√©cup√©ration des donn√©es pr√©trait√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbbaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration des variables d'environnement Python\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "\n",
    "# Configurez la variable d'environnement HADOOP_HOME\n",
    "os.environ['HADOOP_HOME'] = r'C:\\hadoop'\n",
    "os.environ['PATH'] = os.environ['HADOOP_HOME'] + r'\\bin;' + os.environ['PATH']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0141201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç V√©rification de la version PySpark...\n",
      "Version PySpark: 3.5.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"üîç V√©rification de la version PySpark...\")\n",
    "print(f\"Version PySpark: {pyspark.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf7ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cr√©er une session Spark simple (sans MongoDB JAR)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB-PySpark-PyMongo\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.NativeIO$Windows.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    # .config(\"spark.hadoop.io.nativeio.NativeIO.disable.native\", \"true\") \\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbd495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîå Connexion √† MongoDB...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connexion √† MongoDB avec PyMongo\n",
    "print(\"\\nüîå Connexion √† MongoDB...\")\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Attrition_Client_Bancaire_db\"]\n",
    "collection = db[\"clients_pretraite\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1426f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total documents dans MongoDB : 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_Index</th>\n",
       "      <th>Gender_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>690e19f332753e87679009a5</td>\n",
       "      <td>619.0</td>\n",
       "      <td>3.761200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690e19f332753e87679009a6</td>\n",
       "      <td>608.0</td>\n",
       "      <td>3.737670</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>690e19f332753e87679009a7</td>\n",
       "      <td>502.0</td>\n",
       "      <td>3.761200</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>690e19f332753e87679009a8</td>\n",
       "      <td>699.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>690e19f332753e87679009a9</td>\n",
       "      <td>850.0</td>\n",
       "      <td>3.784190</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  CreditScore       Age  Tenure    Balance  \\\n",
       "0  690e19f332753e87679009a5        619.0  3.761200       2       0.00   \n",
       "1  690e19f332753e87679009a6        608.0  3.737670       1   83807.86   \n",
       "2  690e19f332753e87679009a7        502.0  3.761200       8  159660.80   \n",
       "3  690e19f332753e87679009a8        699.0  3.688879       1       0.00   \n",
       "4  690e19f332753e87679009a9        850.0  3.784190       2  125510.82   \n",
       "\n",
       "   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \\\n",
       "0              1          1               1        101348.88       1   \n",
       "1              1          0               1        112542.58       0   \n",
       "2              3          1               0        113931.57       1   \n",
       "3              2          0               0         93826.63       0   \n",
       "4              1          1               1         79084.10       0   \n",
       "\n",
       "   Geography_Index  Gender_Index  \n",
       "0                0             1  \n",
       "1                2             1  \n",
       "2                0             1  \n",
       "3                0             1  \n",
       "4                2             1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Compter les documents\n",
    "total_docs = collection.count_documents({})\n",
    "print(f\"üìä Total documents dans MongoDB : {total_docs}\")\n",
    "\n",
    "# Lire les donn√©es\n",
    "data = list(collection.find())\n",
    "\n",
    "# Convertir en DataFrame Pandas\n",
    "df_pandas = pd.DataFrame(data)\n",
    "\n",
    "display(df_pandas.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c94e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es charg√©es dans Pandas : 10000 lignes\n",
      "üìã Colonnes : ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited', 'Geography_Index', 'Gender_Index']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Supprimer le champ _id \n",
    "if '_id' in df_pandas.columns:\n",
    "    df_pandas = df_pandas.drop('_id', axis=1)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es charg√©es dans Pandas : {len(df_pandas)} lignes\")\n",
    "print(f\"üìã Colonnes : {list(df_pandas.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f276eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Donn√©es converties en Spark DataFrame :\n",
      "root\n",
      " |-- CreditScore: double (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Tenure: long (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: long (nullable = true)\n",
      " |-- HasCrCard: long (nullable = true)\n",
      " |-- IsActiveMember: long (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: long (nullable = true)\n",
      " |-- Geography_Index: long (nullable = true)\n",
      " |-- Gender_Index: long (nullable = true)\n",
      "\n",
      "+-----------+------------------+------+---------+-------------+---------+--------------+---------------+------+---------------+------------+\n",
      "|CreditScore|               Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|Geography_Index|Gender_Index|\n",
      "+-----------+------------------+------+---------+-------------+---------+--------------+---------------+------+---------------+------------+\n",
      "|      619.0|3.7612001156935624|     2|      0.0|            1|        1|             1|      101348.88|     1|              0|           1|\n",
      "|      608.0|3.7376696182833684|     1| 83807.86|            1|        0|             1|      112542.58|     0|              2|           1|\n",
      "|      502.0|3.7612001156935624|     8| 159660.8|            3|        1|             0|      113931.57|     1|              0|           1|\n",
      "|      699.0|3.6888794541139363|     1|      0.0|            2|        0|             0|       93826.63|     0|              0|           1|\n",
      "|      850.0| 3.784189633918261|     2|125510.82|            1|        1|             1|        79084.1|     0|              2|           1|\n",
      "+-----------+------------------+------+---------+-------------+---------+--------------+---------------+------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üìä Nombre total de lignes : 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convertir en DataFrame Spark\n",
    "df = spark.createDataFrame(df_pandas)\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es converties en Spark DataFrame :\")\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "print(f\"\\nüìä Nombre total de lignes : {df.count()}\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc273e",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Gestion du d√©s√©quilibre de classes (undersampling / SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a15b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant SMOTE :\n",
      "Classe 0 : 7963\n",
      "Classe 1 : 2037\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = df.select(\"Exited\", \"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\",\n",
    "                 \"Tenure\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"Geography_Index\", \"Gender_Index\" ).collect()\n",
    "\n",
    "# display(data)\n",
    "\n",
    "\n",
    "X = np.array([[row[\"CreditScore\"], row[\"Age\"], row[\"Balance\"], row[\"EstimatedSalary\"],\n",
    "               row[\"Tenure\"], row[\"NumOfProducts\"], row[\"HasCrCard\"], row[\"IsActiveMember\"],row[\"Geography_Index\"], row[\"Gender_Index\"] ]\n",
    "              for row in data])\n",
    "\n",
    "y = np.array([row[\"Exited\"] for row in data])\n",
    "\n",
    "print(\"Avant SMOTE :\")\n",
    "print(\"Classe 0 :\", sum(y == 0))\n",
    "print(\"Classe 1 :\", sum(y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff141e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apr√®s SMOTE :\n",
      "Classe 0 : 7963\n",
      "Classe 1 : 7963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "smote = SMOTE(random_state=42, sampling_strategy=\"auto\")\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"\\nApr√®s SMOTE :\")\n",
    "unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f\"Classe {cls} : {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4cdea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 15926\n",
      "\n",
      "‚úÖ Donn√©es √©quilibr√©es :\n",
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|     0| 7963|\n",
      "|     1| 7963|\n",
      "+------+-----+\n",
      "\n",
      "\n",
      "üìà Statistiques descriptives :\n",
      "+-------+------------------+-----------------+-------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+\n",
      "|summary|            Exited|      CreditScore|                Age|          Balance|   EstimatedSalary|            Tenure|     NumOfProducts|          HasCrCard|     IsActiveMember|   Geography_Index|      Gender_Index|\n",
      "+-------+------------------+-----------------+-------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+\n",
      "|  count|             15926|            15926|              15926|            15926|             15926|             15926|             15926|              15926|              15926|             15926|             15926|\n",
      "|   mean|               0.5|649.5956485945403| 3.7094709363406992|81920.97404269397|100526.31316938631| 4.834798442797941|1.4217003641843526| 0.6248901167901545|0.37002386035413787|0.6756875549416049|0.4030516137134246|\n",
      "| stddev|0.5000156983406893|91.01054685774102|0.23774903309582415|61402.19270678156| 57721.62205001069|2.7265581975421087|0.5764537477862026|0.48416647744120644| 0.4828258908498153|0.7554273005123634|0.4905263690976535|\n",
      "|    min|                 0|            433.0|  3.031275169010721|              0.0|             11.58|                 0|                 1|                  0|                  0|                 0|                 0|\n",
      "|    max|                 1|            850.0| 4.2718948822260785|        250898.09|         199992.48|                10|                 4|                  1|                  1|                 2|                 1|\n",
      "+-------+------------------+-----------------+-------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the resampled data with proper column names\n",
    "resampled_data = [\n",
    "    Row(\n",
    "        Exited=int(y_resampled[i]),\n",
    "        CreditScore=float(X_resampled[i][0]),\n",
    "        Age=float(X_resampled[i][1]),\n",
    "        Balance=float(X_resampled[i][2]),\n",
    "        EstimatedSalary=float(X_resampled[i][3]),\n",
    "        Tenure=int(X_resampled[i][4]),\n",
    "        NumOfProducts=int(X_resampled[i][5]),\n",
    "        HasCrCard=int(X_resampled[i][6]),\n",
    "        IsActiveMember=int(X_resampled[i][7]),\n",
    "        Geography_Index=int(X_resampled[i][8]),\n",
    "        Gender_Index=int(X_resampled[i][9])\n",
    "    )\n",
    "    for i in range(len(y_resampled))\n",
    "]\n",
    "\n",
    "# Create pandas DataFrame\n",
    "df_pd = pd.DataFrame(resampled_data)\n",
    "\n",
    "# Create Spark DataFrame directly from the list of Row objects\n",
    "df_smote = spark.createDataFrame(resampled_data)\n",
    "\n",
    "\n",
    "print(f\"Total count: {df_smote.count()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es √©quilibr√©es :\")\n",
    "df_smote.groupBy(\"Exited\").count().show()\n",
    "\n",
    "print(\"\\nüìà Statistiques descriptives :\")\n",
    "df_smote.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ded36",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ S√©lection et assemblage des features (VectorAssembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    " 'CreditScore',\n",
    " 'Age',\n",
    " 'Balance',\n",
    " 'EstimatedSalary',\n",
    " 'Tenure',\n",
    " 'NumOfProducts',\n",
    " 'HasCrCard',\n",
    " 'IsActiveMember',\n",
    " 'Geography_Index',\n",
    " 'Gender_Index'\n",
    "]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621aa3f",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Normalisation des features (StandardScaler ou MinMaxScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cr√©ation du StandardScaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"assembled_features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13193c",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Choix du mod√®le de mlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c774bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_m = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"Exited\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae759436",
   "metadata": {},
   "source": [
    "### 6Ô∏è‚É£ Construction d‚Äôun Pipeline int√©grant toutes les √©tapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0962e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr_m])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d7042",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚ûï Split et entra√Ænement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6dc1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_smote.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "273e161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_train = train_df.count()\n",
    "nbr_test = test_df.count()\n",
    "total = df_smote.count()\n",
    "\n",
    "pourcentage_train = (nbr_train / total) * 100\n",
    "pourcentage_test = (nbr_test / total) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efd4f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 12756 - 80.10%\n",
      "test : 3170 - 19.90%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"train : {nbr_train} - {pourcentage_train:.2f}%\")\n",
    "print(f\"test : {nbr_test} - {pourcentage_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4f72d",
   "metadata": {},
   "source": [
    "## üü¢ Entra√Ænement et Validation Crois√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce24d6",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ D√©finition de la grille d‚Äôhyperparam√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00b0e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr_m.regParam, [0.01, 0.05 , 0.1, 0.2, 0.5])\\\n",
    "    .addGrid(lr_m.elasticNetParam, [0.0,0.3, 0.5, 0.7, 1.0])\\\n",
    "    .addGrid(lr_m.maxIter, [5])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Exited\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "    \n",
    "    \n",
    "crossval = (\n",
    "    CrossValidator()\n",
    "    .setEstimator(pipeline_lr)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setNumFolds(3)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0cefb",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Entra√Ænement avec CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfca7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba5fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cv_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d024ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC sur le test set : 0.8290\n"
     ]
    }
   ],
   "source": [
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC sur le test set : {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4832dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur regParam : 0.05\n",
      "Meilleur elasticNetParam : 0.0\n"
     ]
    }
   ],
   "source": [
    "best_model = cv_model.bestModel\n",
    "best_lr = best_model.stages[-1]\n",
    "\n",
    "print(\"Meilleur regParam :\", best_lr._java_obj.getRegParam())\n",
    "print(\"Meilleur elasticNetParam :\", best_lr._java_obj.getElasticNetParam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "805b14e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ √âvaluation du mod√®le :\n",
      "AUC-ROC : 0.8290\n",
      "Accuracy : 0.7372\n",
      "Precision : 0.7377\n",
      "Recall : 0.7372\n",
      "F1-score : 0.7370\n"
     ]
    }
   ],
   "source": [
    "# --- Accuracy ---\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Exited\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "\n",
    "# --- Pr√©cision ---\n",
    "evaluator_prec = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Exited\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision = evaluator_prec.evaluate(predictions)\n",
    "\n",
    "# --- Rappel ---\n",
    "evaluator_rec = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Exited\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "recall = evaluator_rec.evaluate(predictions)\n",
    "\n",
    "# --- F1-score ---\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Exited\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(f\"‚úÖ √âvaluation du mod√®le :\")\n",
    "print(f\"AUC-ROC : {auc:.4f}\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision : {precision:.4f}\")\n",
    "print(f\"Recall : {recall:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "346eac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------------+---------+---------------+------+-------------+---------+--------------+---------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Exited|CreditScore|               Age|  Balance|EstimatedSalary|Tenure|NumOfProducts|HasCrCard|IsActiveMember|Geography_Index|Gender_Index|  assembled_features|     scaled_features|       rawPrediction|         probability|prediction|\n",
      "+------+-----------+------------------+---------+---------------+------+-------------+---------+--------------+---------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     0|      433.0|3.4011973816621555|141325.56|        93839.3|     1|            1|        1|             1|              0|           0|[433.0,3.40119738...|[-2.3812490335084...|[1.22943310972841...|[0.77371933981225...|       0.0|\n",
      "|     0|      433.0|3.7376696182833684|122189.66|       98301.61|    10|            2|        1|             0|              1|           1|[433.0,3.73766961...|[-2.3812490335084...|[-0.0116917019638...|[0.49707710780456...|       1.0|\n",
      "|     0|      438.0|3.6109179126442243|      0.0|        64420.5|     4|            2|        1|             0|              0|           1|[438.0,3.61091791...|[-2.3263370995501...|[0.40306938871043...|[0.59942488983935...|       0.0|\n",
      "|     0|      461.0|3.5553480614894135| 63663.93|      167784.28|     5|            1|        0|             1|              1|           1|[461.0,3.55534806...|[-2.0737422033420...|[0.93631433835501...|[0.71835457010400...|       0.0|\n",
      "|     0|      475.0|3.4965075614664802|119023.28|      100816.29|     8|            1|        1|             0|              0|           1|[475.0,3.49650756...|[-1.9199887882587...|[0.05540504686798...|[0.51384771951399...|       0.0|\n",
      "+------+-----------+------------------+---------+---------------+------+-------------+---------+--------------+---------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d996a45",
   "metadata": {},
   "source": [
    "* matrice de confusion : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "341082a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== üîç Matrice de confusion =====\n",
      "+------+----------+-----+\n",
      "|Exited|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0| 1124|\n",
      "|     0|       1.0|  455|\n",
      "|     1|       0.0|  378|\n",
      "|     1|       1.0| 1213|\n",
      "+------+----------+-----+\n",
      "\n",
      "\n",
      "===== üîç Matrice de confusion =====\n",
      "+------+---------+---------+\n",
      "|Exited|Pr√©dit: 0|Pr√©dit: 1|\n",
      "+------+---------+---------+\n",
      "|     0|     1124|      455|\n",
      "|     1|      378|     1213|\n",
      "+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = (\n",
    "    predictions.groupBy(\"Exited\", \"prediction\")\n",
    "    .count()\n",
    "    .orderBy(\"Exited\", \"prediction\")\n",
    ")\n",
    "\n",
    "print(\"\\n===== üîç Matrice de confusion =====\")\n",
    "confusion.show()\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix = (\n",
    "    predictions.groupBy(\"Exited\")\n",
    "    .pivot(\"prediction\", [0.0, 1.0]) \n",
    "    .agg(F.count(\"prediction\"))\n",
    "    .withColumnRenamed(\"0.0\", \"Pr√©dit: 0\")\n",
    "    .withColumnRenamed(\"1.0\", \"Pr√©dit: 1\")\n",
    "    .orderBy(\"Exited\")\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n===== üîç Matrice de confusion =====\")\n",
    "confusion_matrix.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5573cdb",
   "metadata": {},
   "source": [
    "* Score par combinaison d'hyperparams : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31fe6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ‚öôÔ∏è Scores pour chaque combinaison test√©e =====\n",
      "Combinaison 1: regParam=0.01, elasticNetParam=0.0 --> AUC=0.8360\n",
      "Combinaison 2: regParam=0.01, elasticNetParam=0.3 --> AUC=0.8360\n",
      "Combinaison 3: regParam=0.01, elasticNetParam=0.5 --> AUC=0.8359\n",
      "Combinaison 4: regParam=0.01, elasticNetParam=0.7 --> AUC=0.8358\n",
      "Combinaison 5: regParam=0.01, elasticNetParam=1.0 --> AUC=0.8354\n",
      "Combinaison 6: regParam=0.05, elasticNetParam=0.0 --> AUC=0.8360\n",
      "Combinaison 7: regParam=0.05, elasticNetParam=0.3 --> AUC=0.8352\n",
      "Combinaison 8: regParam=0.05, elasticNetParam=0.5 --> AUC=0.8331\n",
      "Combinaison 9: regParam=0.05, elasticNetParam=0.7 --> AUC=0.8314\n",
      "Combinaison 10: regParam=0.05, elasticNetParam=1.0 --> AUC=0.8268\n",
      "Combinaison 11: regParam=0.1, elasticNetParam=0.0 --> AUC=0.8359\n",
      "Combinaison 12: regParam=0.1, elasticNetParam=0.3 --> AUC=0.8329\n",
      "Combinaison 13: regParam=0.1, elasticNetParam=0.5 --> AUC=0.8285\n",
      "Combinaison 14: regParam=0.1, elasticNetParam=0.7 --> AUC=0.8191\n",
      "Combinaison 15: regParam=0.1, elasticNetParam=1.0 --> AUC=0.8122\n",
      "Combinaison 16: regParam=0.2, elasticNetParam=0.0 --> AUC=0.8356\n",
      "Combinaison 17: regParam=0.2, elasticNetParam=0.3 --> AUC=0.8263\n",
      "Combinaison 18: regParam=0.2, elasticNetParam=0.5 --> AUC=0.8124\n",
      "Combinaison 19: regParam=0.2, elasticNetParam=0.7 --> AUC=0.8120\n",
      "Combinaison 20: regParam=0.2, elasticNetParam=1.0 --> AUC=0.5000\n",
      "Combinaison 21: regParam=0.5, elasticNetParam=0.0 --> AUC=0.8349\n",
      "Combinaison 22: regParam=0.5, elasticNetParam=0.3 --> AUC=0.8120\n",
      "Combinaison 23: regParam=0.5, elasticNetParam=0.5 --> AUC=0.5000\n",
      "Combinaison 24: regParam=0.5, elasticNetParam=0.7 --> AUC=0.5000\n",
      "Combinaison 25: regParam=0.5, elasticNetParam=1.0 --> AUC=0.5000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== ‚öôÔ∏è Scores pour chaque combinaison test√©e =====\")\n",
    "param_combinations = paramGrid\n",
    "metrics = cv_model.avgMetrics\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    reg_param = params[lr_m.regParam]\n",
    "    elastic_param = params[lr_m.elasticNetParam]\n",
    "    auc_score = metrics[i]\n",
    "    print(f\"Combinaison {i+1}: regParam={reg_param}, elasticNetParam={elastic_param} --> AUC={auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1a688",
   "metadata": {},
   "source": [
    "## üü¢ Sauvegarde et D√©ploiement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5e09b",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Sauvegarde du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c78885",
   "metadata": {},
   "source": [
    "* Sauvegarder le mod√®le optimis√© au format natif Spark (model.write().save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4798662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erreur lors de la sauvegarde : An error occurred while calling o32685.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\n",
      "\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n",
      "\t... 68 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = \"../models/best_lr_model\"\n",
    "\n",
    "# Supprimer l'ancien mod√®le s'il existe\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "try:\n",
    "    best_model.write().overwrite().save(model_path)\n",
    "    print(f\"‚úÖ Mod√®le sauvegard√© avec succ√®s dans : {model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la sauvegarde : {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45a48c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå √âchec avec file:// : An error occurred while calling o32774.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\n",
      "\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n",
      "\t... 68 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path_file = f\"file:///{os.path.abspath(model_path).replace(os.sep, '/')}\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "try:\n",
    "    best_model.write().overwrite().save(model_path_file)\n",
    "    print(f\"‚úÖ Mod√®le sauvegard√© avec file:// : {model_path_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå √âchec avec file:// : {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd5be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
